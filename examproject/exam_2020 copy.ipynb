{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy import optimize\n",
    "import sympy as sm\n",
    "import scipy as sp\n",
    "from scipy import interpolate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following **linear equation:**\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have access to data of the **independent variables** ($x_{1,i}$, $x_{2,i}$) and the **dependent variable** ($y_i$) for $N$ individuals, where $i$ indexes individuals. The variable $\\epsilon_i$ is a mean-zero **stochastic shock**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the **data generating process** is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP(N):\n",
    "    \n",
    "    # a. independent variables\n",
    "    x1 = np.random.normal(0,1,size=N)\n",
    "    x2 = np.random.normal(0,1,size=N)\n",
    "    \n",
    "    # b. errors\n",
    "    eps = np.random.normal(0,1,size=N)\n",
    "    \n",
    "    extreme = np.random.uniform(0,1,size=N)\n",
    "    eps[extreme < 0.05] += np.random.normal(-5,1,size=N)[extreme < 0.05]\n",
    "    eps[extreme > 0.95] += np.random.normal(5,1,size=N)[extreme > 0.95]\n",
    "    \n",
    "    # c. dependent variable\n",
    "    y = 0.1 + 0.3*x1 + 0.5*x2 + eps\n",
    "    \n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data you have access to is:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "x1,x2,y = DGP(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Estimate the vector of coefficients $\\mathbf{\\beta} = (\\beta_0,\\beta_1,\\beta_2)$ using **ordinary least squares (OLS)** implemented with **matrix algebra** by\n",
    "\n",
    "$$ \\hat{\\mathbf{\\beta}} = (\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\mathbf{y} $$\n",
    "\n",
    "where $\\mathbf{X}^{\\prime}$ is the transpose of $\\mathbf{X}$ and\n",
    "\n",
    "$$\\mathbf{y} = \n",
    "\\pmatrix{ y_1 \\cr y_2 \\cr  \\vdots \\cr y_N \n",
    "}\n",
    ", \\quad \\mathbf{X} = \\pmatrix{\n",
    "1 & x_{1,1} & x_{2,1} \\cr \n",
    "1 & x_{1,2} & x_{2,2} \\cr \n",
    "\\vdots & \\vdots \\cr \n",
    "1 & x_{1,N} & x_{2,N} \n",
    "}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10000,)\n(10000,)\n(10000,)\n(10000,)\n(10000, 3)\n[[ 1.         -1.76884571 -0.18279442]\n [ 1.          0.07555227  0.78062368]\n [ 1.         -1.1306297  -1.01220533]\n ...\n [ 1.          0.0370484  -1.44286811]\n [ 1.          1.70892684 -0.10668645]\n [ 1.          2.06128052  0.55908184]]\n"
    }
   ],
   "source": [
    "#Q1 Answer\n",
    "#coeffs = linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "Y=np.array(y)\n",
    "X1=np.array(x1)\n",
    "X2=np.array(x2)\n",
    "print(y.shape)\n",
    "print(X1.shape)\n",
    "print(X2.shape)\n",
    "X0=np.ones(shape=y.shape)\n",
    "print(X0.shape)\n",
    "X=np.column_stack((X0,X1))\n",
    "X=np.column_stack((X,X2))\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.0956821  0.29294299 0.50332771]\n"
    }
   ],
   "source": [
    "coeffs = linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.09568210492389777\n0.2929429877107508\n0.5033277126888069\n"
    }
   ],
   "source": [
    "# we end up with the results:\n",
    "B_0_hat=coeffs[0]\n",
    "print(B_0_hat)\n",
    "B_1_hat=coeffs[1]\n",
    "print(B_1_hat)\n",
    "B_2_hat=coeffs[2]\n",
    "print(B_2_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Construct a 3D plot, where the data is plotted as scattered points, and the prediction of the model is given by the plane\n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1,i} + \\hat{\\beta}_2 x_{2,i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Esimtate the vector of coefficients $\\mathbf{\\beta} = (\\beta_0,\\beta_1,\\beta_2)$ using a **numerical solver** to solve the ordinary least square problem, shown below, directly. Compare your results with the matrix algebra results.\n",
    "\n",
    "$$ \\min_{\\mathbf{\\beta}} \\sum^N_{i=1} (y_i - (\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}) )^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Estimate the vector of coefficients $\\mathbf{\\beta} = (\\beta_0,\\beta_1,\\beta_2)$ using **least absolute deviations (LAD)** using a numerical solver to solve the following problem directly: \n",
    "\n",
    "$$  \\min_{\\beta} \\sum^N_{i=1} |y_i - (\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}) | $$\n",
    "\n",
    "where $|z|$ is the absolute value of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Set $N = 50$. Repeat the estimation using the **OLS** and **LAD** methods $K=5000$ times, drawing a new random sample from the data generating process each time. Compare the estimates from each method using histograms. Which method do you prefer? Explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durable purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a **household** living in two periods.\n",
    "\n",
    "In the **second period** it gets utility from **non-durable consumption**, $c$, and **durable consumption**, $d+\\chi x$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{2}(m_{2},d)&= \\max_{c}\\frac{(c^{\\alpha}(d+\\chi x)^{1-\\alpha})^{1-\\rho}}{1-\\rho}\\\\\n",
    "\\text{s.t.} \\\\\n",
    "x &= m_{2}-c \\\\\n",
    "c &\\in [0,m_{2}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "* $m_2$ is cash-on-hand in the beginning of period 2\n",
    "* $c$ is non-durable consumption\n",
    "* $d$ is pre-commited durable consumption\n",
    "* $x = m_2 - c$ is extra durable consumption\n",
    "* $\\rho > 1$ is the risk aversion coefficient\n",
    "* $\\alpha \\in (0,1)$ is the utility weight on non-durable consumption\n",
    "* $\\chi \\in (0,1)$ implies that extra durable consumption is *less* valuable than pre-comitted durable consumption\n",
    "* the second constraint ensures the household *cannot* die in debt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **value function** $v_2(m_2,d)$ measures the household's value of having $m_2$ at the beginning of period 2 with precomitted durable consumption of $d$. The optimal choice of non-durable consumption is denoted $c^{\\ast}(m_2,d)$. The optimal extra durable consumption function is $x^{\\ast}(m_2,d) = m_2-c^{\\ast}(m_2,d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the so-called **end-of-period 1 value function** as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w(a,d)&\\equiv\\beta\\mathbb{E}_{1}\\left[v_2(m_2,d)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_2&= (1+r)a+y \\\\\n",
    "y &= \\begin{cases}\n",
    "1-\\Delta & \\text{with prob. }\\frac{1}{3}\\\\\n",
    "1 & \\text{with prob. }\\frac{1}{3}\\\\\n",
    "1+\\Delta & \\text{with prob. }\\frac{1}{3}\n",
    "\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "* $a$ is assets at the end of period 1\n",
    "* $\\beta > 0$ is the discount factor\n",
    "* $\\mathbb{E}_1$ is the expectation operator conditional on information in period 1\n",
    "* $y$ is income in period 2\n",
    "* $\\Delta \\in (0,1)$ is the level of income risk (mean-preserving)\n",
    "* $r$ is the return on savings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **first period**, the household chooses it's pre-comitted level of durable consumption for the next-period,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{1}(m_{1})&=\\max_{d} w(a,d)\\\\&\\text{s.t.}&\\\\\n",
    "a&= m_{1}-d \\\\\n",
    "d&\\in [0,m_{1}]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $m_1$ is cash-on-hand in period 1. The second constraint ensures the household *cannot* borrow. The **value function** $v_1(m_1)$ measures the household's value of having $m_1$ at the beginning of period 1. The optimal choice of pre-committed durable consumption is denoted $d^{\\ast}(m_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **parameters** and **grids** for $m_1$, $m_2$ and $d$ should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. parameters\n",
    "rho = 2\n",
    "alpha = 0.8\n",
    "beta = 0.96\n",
    "r = 0.04\n",
    "Delta = 0.25\n",
    "\n",
    "# b. grids\n",
    "m1_vec = np.linspace(1e-8,10,100)\n",
    "m2_vec = np.linspace(1e-8,10,100)\n",
    "d_vec = np.linspace(1e-8,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally we received information on the parameter chi, which should be set to 0.9:\n",
    "chi=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining value function 2 and substituting the constraint into the value function\n",
    "def v2(c,rho,d,chi,m2,alpha):\n",
    "    return (c**alpha*(d+chi**(m2-c))**(1-alpha))**(1-rho)/(1-rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-135-c5278e1bf706>, line 3)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-135-c5278e1bf706>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    def v2(c,alpha,d,rho,m2,chi)\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Defining basic functions\n",
    "\n",
    "def v2(c,alpha,d,rho,m2,chi)\n",
    "    return (c**alpha*(d+chi**(m2-c))**(1-alpha))**(1-rho)/(1-rho) #Have substituted the constraint into the function\n",
    "\n",
    "def w(beta,r,a,y,d,Delta,v2_interp):\n",
    "   \n",
    "    # a. w value if low income\n",
    "    m2_low = (1+r)*(m1-c1)*a + 1-Delta\n",
    "    v2_low = v2_interp([m2_low])[0] \n",
    "\n",
    "    # b. w value if mid income\n",
    "    m2_mid = (1+r)*(m1-c1)*a + 1\n",
    "    v2_mid = v2_interp([m2_mid])[0] \n",
    "    \n",
    "    # c. w value if high income\n",
    "    m2_high = (1+r)*(m1-c1)*a + 1+Delta\n",
    "    v2_high = v2_interp([m2_high])[0] \n",
    "\n",
    "    # d. expected w value\n",
    "    w = ((1/3)*v2_low + (1/3)*v2_mid + (1/3)*v2_high)*beta\n",
    "    \n",
    "    # e. final w\n",
    "    return w\n",
    "\n",
    "def v1():\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining end-of-period 1 value function\n",
    "def w(beta,r,a,y,d,Delta,v2_interp):\n",
    "\n",
    "    # a. w value if low income\n",
    "    m2_low = (1+r)*(m1-c1)*a + 1-Delta\n",
    "    v2_low = v2_interp([m2_low])[0] \n",
    "\n",
    "    # b. w value if mid income\n",
    "    m2_mid = (1+r)*(m1-c1)*a + 1\n",
    "    v2_mid = v2_interp([m2_mid])[0] \n",
    "    \n",
    "    # c. w value if high income\n",
    "    m2_high = (1+r)*(m1-c1)*a + 1+Delta\n",
    "    v2_high = v2_interp([m2_high])[0] \n",
    "\n",
    "    # d. expected w value\n",
    "    = ((1/3)*v2_low + (1/3)*v2_mid + (1/3)*v2_high)*beta\n",
    "\n",
    "    return w(a,d)\n",
    "## Lidt i tvivl om hvor der skal bruges w, v1, eller utility. Se afsnit 5 her: \n",
    "##https://numeconcopenhagen.netlify.app/lectures/Numerical_optimization\n",
    "##SE PROBLEM SET 7! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Find and plot the functions $v_{2}(m_{2},d)$, $c^{\\ast}(m_2,d)$, and $x^{\\ast}(m_2,d)$. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-b37e89c48180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# solve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mm1_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolve_period_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# illustration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-b37e89c48180>\u001b[0m in \u001b[0;36msolve_period_2\u001b[1;34m(rho, alpha, beta, r, Delta)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# iii. optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bounded'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# iv. save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize_scalar\u001b[1;34m(fun, bracket, bounds, args, method, tol, options)\u001b[0m\n\u001b[0;32m    778\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'disp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_scalar_bounded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'golden'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_scalar_golden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbracket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_scalar_bounded\u001b[1;34m(func, bounds, args, xatol, maxiter, disp, **unknown_options)\u001b[0m\n\u001b[0;32m   1772\u001b[0m     \u001b[0mrat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1774\u001b[1;33m     \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1775\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m     \u001b[0mfmin_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-b37e89c48180>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# i. objective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# ii. initial value (consume half)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "def solve_period_2(rho, alpha, beta, r, Delta):\n",
    "\n",
    "    # a. grids\n",
    "    m1_vec = np.linspace(1e-8,10,100)\n",
    "    m2_vec = np.linspace(1e-8,10,100)\n",
    "    d_vec = np.linspace(1e-8,5,100)\n",
    "\n",
    "    # b. solve for each m2 in grid\n",
    "    for i,m2 in enumerate(m2_vec):\n",
    "\n",
    "        # i. objective\n",
    "        obj = lambda d: -m2(rho, alpha, beta, r, Delta)\n",
    "\n",
    "        # ii. initial value (consume half)\n",
    "        x0 = m2/3\n",
    "\n",
    "        # iii. optimizer\n",
    "        result = optimize.minimize_scalar(obj,x0,method='bounded',bounds=[1e-8,m2])\n",
    "\n",
    "        # iv. save\n",
    "        m2_vec[i] = -result.fun\n",
    "        d_vec[i] = result.x\n",
    "        \n",
    "    return m1_vec,m2_vec,d_vec\n",
    "\n",
    "# solve\n",
    "m1_vec,m2_vec,d_vec = solve_period_2(rho, alpha, beta, r, Delta)\n",
    "\n",
    "# illustration\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(m2_vec,d_vec)\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('$d_2$')\n",
    "ax.set_title('consumption function in period 2')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(m2_vec,m1_vec)\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('$m_1$')\n",
    "ax.set_title('value function in period 2')\n",
    "ax.set_ylim([-40,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Find and plot the functions $v_{1}(m_{1})$ and $d^{\\ast}(m_1)$. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_interp = interpolate.RegularGridInterpolator([m2_vec], d_vec,\n",
    "                                                bounds_error=False,fill_value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** For interpolation of $v_2(m_2,d)$ consider using `interpolate.RegularGridInterpolator([GRID-VECTOR1,GRID-VECTOR2],VALUE-MATRIX,bounds_error=False,fill_value=None)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider an **extension** of the model, where there is also a **period 0**. In this period, the household makes a choice whether to stick with the level of durables it has, $z = 0$, or adjust its stock of durables, $z = 1$. If adjusting, the household loses a part of the value of its durable stock; more specificaly it incurs a proportional loss of $\\Lambda \\in (0,1)$.\n",
    "\n",
    "Mathematically, the **household problem in period 0** is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{0}(m_{0},d_{0}) &= \\max_{z\\in\\{0,1\\}} \\begin{cases}\n",
    "w(m_{0},d_{0}) & \\text{if } z = 0\\\\\n",
    "v_1(m_0+(1-\\Lambda) d_{0}) & \\text{if } z = 1\\\\\n",
    "\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **parameters** and **grids** for $m_0$ and $d_0$ should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0.2\n",
    "m0_vec = np.linspace(1e-8,6,100)\n",
    "d0_vec = np.linspace(1e-8,3,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** For which values of $m_0$ and  $d_0$ is the optimal choice not to adjust, i.e. $z = 0$? Show this in a plot. Give an interpretion of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\boldsymbol{x} = \\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2\\\\\n",
    "\\end{array}\\right]$ be a two-dimensional vector. Consider the following algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** `gradient_descent()`\n",
    "\n",
    "**Goal:** Minimize the function $f(\\boldsymbol{x})$.\n",
    "\n",
    "1. Choose a tolerance $\\epsilon>0$, a scale factor $ \\Theta > 0$, and a small number $\\Delta > 0$\n",
    "2. Guess on $\\boldsymbol{x}_0$ and set $n=1$\n",
    "3. Compute a numerical approximation of the jacobian for $f$ by\n",
    "\n",
    "    $$\n",
    "    \\nabla f(\\boldsymbol{x}_{n-1}) \\approx \\frac{1}{\\Delta}\\left[\\begin{array}{c}\n",
    "    f\\left(\\boldsymbol{x}_{n-1}+\\left[\\begin{array}{c}\n",
    "    \\Delta\\\\\n",
    "    0\n",
    "    \\end{array}\\right]\\right)-f(\\boldsymbol{x}_{n-1})\\\\\n",
    "    f\\left(\\boldsymbol{x}_{n-1}+\\left[\\begin{array}{c}\n",
    "    0\\\\\n",
    "    \\Delta\n",
    "    \\end{array}\\right]\\right)-f(\\boldsymbol{x}_{n-1})\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "\n",
    "4. Stop if the maximum element in $|\\nabla f(\\boldsymbol{x}_{n-1})|$ is less than $\\epsilon$\n",
    "5. Set $\\theta = \\Theta$ \n",
    "6. Compute $f^{\\theta}_{n} = f(\\boldsymbol{x}_{n-1} - \\theta \\nabla f(\\boldsymbol{x}_{n-1}))$\n",
    "7. If $f^{\\theta}_{n} < f(\\boldsymbol{x}_{n-1})$ continue to step 9\n",
    "8. Set $\\theta = \\frac{\\theta}{2}$ and return to step 6     \n",
    "9. Set $x_{n} = x_{n-1} - \\theta \\nabla f(\\boldsymbol{x}_{n-1})$\n",
    "10. Set $n = n + 1$ and return to step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Implement the algorithm above such that the code below can run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,x0,epsilon=1e-6,Theta=0.1,Delta=1e-8,max_iter=10_000):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = sm.symbols('x_1')\n",
    "x2 = sm.symbols('x_2')\n",
    "f = (1.0-x1)**2+2*(x2-x1**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Matrix([\n[-8*x_1*(-x_1**2 + x_2) + 2*x_1 - 2.0],\n[                   -4*x_1**2 + 4*x_2]])",
      "text/latex": "$\\displaystyle \\left[\\begin{matrix}- 8 x_{1} \\left(- x_{1}^{2} + x_{2}\\right) + 2 x_{1} - 2.0\\\\- 4 x_{1}^{2} + 4 x_{2}\\end{matrix}\\right]$"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "Df = sm.Matrix([sm.diff(f,i) for i in [x1,x2]])\n",
    "Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Matrix([\n[2*(12*x_1**2 - 4*x_2 + 1), -8*x_1],\n[                   -8*x_1,      4]])",
      "text/latex": "$\\displaystyle \\left[\\begin{matrix}2 \\left(12 x_{1}^{2} - 4 x_{2} + 1\\right) & - 8 x_{1}\\\\- 8 x_{1} & 4\\end{matrix}\\right]$"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "Hf = sm.Matrix([[sm.diff(f,i,j) for j in [x1,x2]] for i in [x1,x2]])\n",
    "Hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,x0,epsilon=1e-6,Theta=0.1,Delta=1e-8,max_iter=10_000,tol=1e-8):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'max_iter' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-3a340d73adf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mx_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfx_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_iter' is not defined"
     ]
    }
   ],
   "source": [
    "def rosen(x):\n",
    "    return (1.0-x[0])**2+2*(x[1]-x[0]**2)**2\n",
    "\n",
    "x0 = np.array([1.1,1.1])\n",
    "n=1\n",
    "\n",
    "while n < max_iter:\n",
    "    x_prev = x\n",
    "    fx_prev = fx\n",
    "\n",
    "try:\n",
    "    x,it = minimize_gradient_descent(rosen,x0)\n",
    "    print(f'minimum found at ({x[0]:.4f},{x[1]:.4f}) after {it} iterations')\n",
    "    assert np.allclose(x,[1,1])\n",
    "except:\n",
    "    print('not implemented yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _rosen(x1,x2):\n",
    "    return (1.0-x[0])**2+2*(x[1]-x[0]**2)**2\n",
    "def rosen(x):\n",
    "    return _rosen(x[0],x[1])\n",
    "def rosen_jac(x):\n",
    "    return np.array([-8*x[0]*(-x[0]+x[1])+2*x[0]-2],[-4*x[0]**2+4*x[1]])\n",
    "def rosen_hess(x):\n",
    "    return np.array([2*(12*x[0]**2-4*x[1]+1)-8*x[0]],[-8*x[0]+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,x0,epsilon=1e-6,Theta=0.1,Delta=1e-8,max_iter=10_000,tol=1e-8):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not implemented yet\n"
    }
   ],
   "source": [
    "def rosen(x):\n",
    "    return (1.0-x[0])**2+2*(x[1]-x[0]**2)**2\n",
    "\n",
    "x0 = np.array([1.1,1.1])\n",
    "\n",
    "    while n < max_iter:\n",
    "        \n",
    "\n",
    "        try:    \n",
    "    x,it = gradient_descent(rosen,x0)\n",
    "    print(f'minimum found at ({x[0]:.4f},{x[1]:.4f}) after {it} iterations')\n",
    "    assert np.allclose(x,[1,1])\n",
    "        except:\n",
    "    print('not implemented yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_gradient_descent(f,x0,jac,alphas=[0.01,0.05,0.1,0.25,0.5,1],max_iter=500,tol=1e-8):\n",
    "    \"\"\" minimize function with gradient descent\n",
    "        \n",
    "    Args:\n",
    "\n",
    "        f (callable): function\n",
    "        x0 (np.ndarray): initial values\n",
    "        jac (callable): jacobian\n",
    "        alpha (list): potential step sizes\n",
    "        max_iter (int): maximum number of iterations\n",
    "        tol (float): tolerance\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        x (np.ndarray): minimum\n",
    "        n (int): number of iterations used\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # step 1: initialize\n",
    "    x = x0\n",
    "    fx = f(x0)\n",
    "    n = 1\n",
    "    \n",
    "    # step 2-6: iteration\n",
    "    while n < max_iter:\n",
    "            \n",
    "        x_prev = x\n",
    "        fx_prev = fx\n",
    "        \n",
    "        # step 2: evaluate gradient\n",
    "        jacx = jac(x)\n",
    "        \n",
    "        # step 3: find good step size (line search)\n",
    "        fx_ast = np.inf\n",
    "        alpha_ast = np.nan\n",
    "        for alpha in alphas:\n",
    "            x = x_prev - alpha*jacx\n",
    "            fx = f(x)\n",
    "            if fx < fx_ast:\n",
    "                fx_ast = fx\n",
    "                alpha_ast = alpha\n",
    "        \n",
    "        # step 4: update guess\n",
    "        x = x_prev - alpha_ast*jacx\n",
    "                            \n",
    "        # step 5: check convergence\n",
    "        fx = f(x)\n",
    "        if abs(fx-fx_prev) < tol:\n",
    "            break\n",
    "            \n",
    "        # d. update i\n",
    "        n += 1\n",
    "        \n",
    "    return x,n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}